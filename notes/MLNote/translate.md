# translate

## 打算翻译两部外国作品，《hacking with Go》 我自己也写了一本差不多的书，但是我的已黑客工具开发为主，涉及的基础知识不是很多，想对内容进行一点修缮，希望得到一点启发。
## 还有一本是关于深度强化学习的。  

### 人工智能术语表

|English Terminology	|中文术语|
|----|----|
neural networks	|神经网络
activation function	|激活函数
hyperbolic tangent	|双曲正切函数
bias units	|偏置项
activation	|激活值
forward propagation	|前向传播
feedforward neural network	|前馈神经网络
Backpropagation Algorithm	|反向传播算法
(batch) gradient descent	|（批量）梯度下降法
(overall) cost function	|（整体）代价函数
squared-error	|方差
average sum-of-squares error|	均方差
regularization term	|规则化项
weight decay	|权重衰减
bias terms	|偏置项
Bayesian regularization method	|贝叶斯规则化方法
Gaussian prior	|高斯先验概率
MAP	|极大后验估计
maximum likelihood estimation	|极大似然估计
activation function	|激活函数
tanh function	|双曲正切函数
non-convex function	|非凸函数
hidden (layer) units	|隐藏层单元
symmetry breaking	|对称失效
learning rate	|学习速率
forward pass	|前向传导
hypothesis	|假设值
error term	|残差
weighted average	|加权平均值
feedforward pass	|前馈传导
Hadamard product	|阿达马乘积
forward propagation	|前向传播
off-by-one error	|缺位错误
bias term	|偏置项
numerically checking	|数值检验
numerical roundoff errors	|数值舍入误差
significant digits	|有效数字
unrolling	|组合扩展
learning rate	|学习速率
Hessian matrix Hessian	|矩阵
Newton's method	|牛顿法
conjugate gradient	|共轭梯度
step-size	|步长值
Autoencoders	|自编码算法
Sparsity	|稀疏性
neural networks	|神经网络
supervised learning	|监督学习
unsupervised learning	|无监督学习
hidden units	|隐藏神经元
the pixel intensity value	|像素灰度值
IID	|独立同分布
PCA	|主元分析
active	|激活
inactive	|抑制
activation function	|激活函数
activation	|激活度
the average activation	|平均活跃度
sparsity parameter	|稀疏性参数
penalty term	|惩罚因子
KL divergence|	KL 散度
Bernoulli random variable	|伯努利随机变量
overall cost function	|总体代价函数
backpropagation	|后向传播
forward pass	|前向传播
gradient descent	|梯度下降
the objective	|目标函数
the derivative checking method	|梯度验证方法
Visualizing	|可视化
Autoencoder	|自编码器
hidden unit	|隐藏单元
non-linear feature	|非线性特征
activate	|激励
trivial answer	|平凡解
norm constrained	|范数约束
sparse autoencoder	|稀疏自编码器
norm bounded	|有界范数
input domains	|输入域
vectorization	|矢量化
Logistic Regression	|逻辑回归
batch gradient ascent	|批量梯度上升法
intercept term	|截距
the log likelihood	|对数似然函数
derivative	|导函数
gradient	|梯度
vectorization	|向量化
forward propagation	|正向传播
backpropagation	|反向传播
training examples	|训练样本
activation function	|激活函数
sparse autoencoder	|稀疏自编码网络
sparsity penalty	|稀疏惩罚
average firing rate	|平均激活率
Principal Components Analysis	|主成份分析
whitening	|白化
intensity	|亮度
mean	|平均值
variance	|方差
covariance matrix	|协方差矩阵
basis	|基
magnitude	|幅值
stationarity	|平稳性
normalization	|归一化
eigenvector	|特征向量
redundant	|冗余
variance	|方差
smoothing	|平滑
dimensionality reduction	|降维
regularization	|正则化
reflection matrix	|反射矩阵
decorrelation	|去相关
Principal Components Analysis (PCA)	|主成分分析
zero-mean	|均值为零
mean value	|均值
eigenvalue|	特征值
symmetric positive semi-definite matrix	|对称半正定矩阵
numerically reliable	|数值计算上稳定
sorted in decreasing order	|降序排列
singular value	|奇异值
singular vector	|奇异向量
vectorized implementation|	向量化实现
diagonal	|对角线
Softmax Regression	|Softmax回归
supervised learning	|有监督学习
unsupervised learning	|无监督学习
deep learning	|深度学习
logistic regression	|logistic回归
intercept term	|截距项
binary classification	|二元分类
class labels	|类型标记
hypothesis	|估值函数/估计值
cost function	|代价函数
multi-class classification	|多元分类
weight decay	|权重衰减
self-taught learning	|自我学习/自学习
unsupervised feature learning	|无监督特征学习
autoencoder	|自编码器
semi-supervised learning	|半监督学习
deep networks	|深层网络
fine-tune	|微调
unsupervised feature learning	|非监督特征学习
pre-training	|预训练
Deep Networks	|深度网络
deep neural networks	|深度神经网络
non-linear transformation	|非线性变换
represent compactly	|简洁地表达
part-whole decompositions	|“部分-整体”的分解
parts of objects	|目标的部件
highly non-convex optimization problem	|高度非凸的优化问题
conjugate gradient	|共轭梯度
diffusion of gradients	|梯度的弥散
Greedy layer-wise training	|逐层贪婪训练方法
autoencoder	|自动编码器
Greedy layer-wise training	|逐层贪婪训练法
Stacked autoencoder	|栈式自编码神经网络
Fine-tuning	|微调
Raw inputs	|原始输入
Hierarchical grouping	|层次型分组
Part-whole decomposition	|部分-整体分解
First-order features	|一阶特征
Second-order features	|二阶特征
Higher-order features	|更高阶特征
Linear Decoders	|线性解码器
Sparse Autoencoder	|稀疏自编码
input layer	|输入层
hidden layer	|隐含层
output layer	|输出层
neuron	|神经元
robust	|鲁棒
sigmoid activation function	|S型激励函数
tanh function	|tanh激励函数
linear activation function	|线性激励函数
identity activation function	|恒等激励函数
hidden unit	|隐单元
weight	|权重
error term	|偏差项
Full Connected Networks	|全连接网络
Sparse Autoencoder	|稀疏编码
Feedforward	|前向输送
Backpropagation	|反向传播
Locally Connected Networks	|部分联通网络
Contiguous Groups	|连接区域
Visual Cortex	|视觉皮层
Convolution	|卷积
Stationary	|固有特征
Pool	|池化
features	|特征
example	|样例
over-fitting	|过拟合
translation invariant	|平移不变性
pooling	|池化
extract	|提取
object detection	|物体检测
DC component	|直流分量
local mean subtraction	|局部均值消减
sparse autoencoder	|消减归一化
rescaling	|缩放
per-example mean subtraction	|逐样本均值消减
feature standardization	|特征标准化
stationary	|平稳
zero-mean	|零均值化
low-pass filtering	|低通滤波
reconstruction based models	|基于重构的模型
RBMs	|受限Boltzman机
k-Means|	k-均值
long tail	|长尾
loss function	|损失函数
orthogonalization	|正交化
Sparse Coding	|稀疏编码
unsupervised method	|无监督学习
over-complete bases	|超完备基
degeneracy	|退化
reconstruction term	|重构项
sparsity penalty	|稀疏惩罚项
norm	|范式
generative model	|生成模型
linear superposition	|线性叠加
additive noise	|加性噪声
basis feature vectors|	特征基向量
the empirical distribution	|经验分布函数
the log-likelihood	|对数似然函数
Gaussian white noise	|高斯白噪音
the prior distribution	|先验分布
prior probability	|先验概率
source features	|源特征
the energy function	|能量函数
regularized	|正则化
least squares	|最小二乘法
convex optimization software	|凸优化软件
conjugate gradient methods	|共轭梯度法
quadratic constraints	|二次约束
the Lagrange dual	|拉格朗日对偶函数
feedforward architectures	|前馈结构算法
Independent Component Analysis	|独立成分分析
Over-complete basis	|超完备基
Orthonormal basis	|标准正交基
Sparsity penalty	|稀疏惩罚项
Under-complete basis	|不完备基
Line-search algorithm	|线搜索算法
Topographic cost term	|拓扑代价项
